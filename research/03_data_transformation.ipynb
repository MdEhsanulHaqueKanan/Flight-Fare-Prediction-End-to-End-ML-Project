{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g:\\\\Machine_Learning_Projects\\\\iNeuron internship\\\\Flight-Fare-Prediction-End-to-End-ML-Project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g:\\\\Machine_Learning_Projects\\\\iNeuron internship\\\\Flight-Fare-Prediction-End-to-End-ML-Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlProject.constants import *\n",
    "from mlProject.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlProject import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "# New Line\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "\n",
    "    # New Function Added\n",
    "    # https://github.com/yash1314/Flight-Price-Prediction/blob/main/src/utils.py\n",
    "    def convert_to_minutes(self, duration):\n",
    "        try:\n",
    "            hours, minute = 0, 0\n",
    "            for i in duration.split():\n",
    "                if 'h' in i:\n",
    "                    hours = int(i[:-1])\n",
    "                elif 'm' in i:\n",
    "                    minute = int(i[:-1])\n",
    "            return hours * 60 + minute\n",
    "        except :\n",
    "            return None \n",
    "\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    \n",
    "    ## Note: You can add different data transformation techniques such as Scaler, PCA and all\n",
    "    #You can perform all kinds of EDA in ML cycle here before passing this data to the model\n",
    "\n",
    "    # I am only adding train_test_spliting cz this data is already cleaned up\n",
    "\n",
    "    # New Code Added Start\n",
    "    def initiate_data_transformation(self):\n",
    "        ## reading the data\n",
    "        # df = pd.read_csv(self.config.data_path)\n",
    "        # New Line\n",
    "        df = pd.read_excel(self.config.data_path)\n",
    "\n",
    "        # I added this line to see the data path\n",
    "        logger.info(f' data path: \\n{self.config.data_path}')\n",
    "        logger.info('Read data completed')\n",
    "        logger.info(f'df dataframe head: \\n{df.head().to_string()}')\n",
    "\n",
    "        ## dropping null values\n",
    "        df.dropna(inplace = True)\n",
    "\n",
    "        ## Date of journey column transformation\n",
    "        df['journey_date'] = pd.to_datetime(df['Date_of_Journey'], format =\"%d/%m/%Y\").dt.day\n",
    "        df['journey_month'] = pd.to_datetime(df['Date_of_Journey'], format =\"%d/%m/%Y\").dt.month\n",
    "\n",
    "        ## encoding total stops.\n",
    "        df.replace({'Total_Stops': {'non-stop' : 0, '1 stop': 1, '2 stops': 2, '3 stops': 3, '4 stops': 4}}, inplace = True)\n",
    "\n",
    "        ## ecoding airline, source, and destination\n",
    "        df_airline = pd.get_dummies(df['Airline'], dtype=int)\n",
    "        df_source = pd.get_dummies(df['Source'],  dtype=int)\n",
    "        df_dest = pd.get_dummies(df['Destination'], dtype=int)\n",
    "\n",
    "        ## dropping first columns of each categorical variables.\n",
    "        df_airline.drop('Trujet', axis = 1, inplace = True)\n",
    "        df_source.drop('Banglore', axis = 1, inplace = True)\n",
    "        df_dest.drop('Banglore', axis = 1, inplace = True)\n",
    "\n",
    "        df = pd.concat([df, df_airline, df_source, df_dest], axis = 1)\n",
    "       \n",
    "        ## handling duration column\n",
    "        # df['duration'] = df['Duration'].apply(convert_to_minutes)\n",
    "        # New Line Added\n",
    "        df['duration'] = df['Duration'].apply(self.convert_to_minutes)\n",
    "        upper_time_limit = df.duration.mean() + 1.5 * df.duration.std()\n",
    "        df['duration'] = df['duration'].clip(upper = upper_time_limit)\n",
    "\n",
    "        ## encodign duration column\n",
    "        bins = [0, 120, 360, 1440]  # custom bin intervals for 'Short,' 'Medium,' and 'Long'\n",
    "        labels = ['Short', 'Medium', 'Long'] # creating labels for encoding\n",
    "\n",
    "        df['duration'] = pd.cut(df['duration'], bins=bins, labels=labels)\n",
    "        df.replace({'duration': {'Short':1, 'Medium':2, 'Long': 3}}, inplace = True)\n",
    "        \n",
    "        ## dropping the columns\n",
    "        cols_to_drop = ['Airline', 'Date_of_Journey', 'Source', 'Destination', 'Route', 'Dep_Time', 'Arrival_Time', 'Duration', 'Additional_Info', 'Delhi', 'Kolkata']\n",
    "        df.drop(cols_to_drop, axis = 1, inplace = True)\n",
    "\n",
    "        logger.info('df data transformation completed')\n",
    "        logger.info(f' transformed df data head: \\n{df.head().to_string()}')\n",
    "        \n",
    "\n",
    "        # df.to_csv(self.data_transformation_config.transformed_data_file_path, index = False, header= True)\n",
    "        # New Line\n",
    "        df.to_excel(self.config.data_path, index = False, header= True)\n",
    "        # df.to_excel(self.config.transformed_data_file_path, index = False, header= True)\n",
    "        # df.to_excel(self.data_transformation_config.transformed_data_file_path, index = False, header= True)\n",
    "        logger.info(\"transformed data is stored\")\n",
    "        df.head(1)\n",
    "        ## splitting the data into training and target data\n",
    "        X = df.drop('Price', axis = 1)\n",
    "        y = df['Price']\n",
    "        \n",
    "        ## accessing the feature importance.\n",
    "        select = ExtraTreesRegressor()\n",
    "        select.fit(X, y)\n",
    "\n",
    "        # plt.figure(figsize=(12, 8))\n",
    "        # fig_importances = pd.Series(select.feature_importances_, index=X.columns)\n",
    "        # fig_importances.nlargest(20).plot(kind='barh')\n",
    "    \n",
    "        # ## specify the path to the \"visuals\" folder using os.path.join\n",
    "        # visuals_folder = 'visuals'\n",
    "        # if not os.path.exists(visuals_folder):\n",
    "        #     os.makedirs(visuals_folder)\n",
    "\n",
    "        # ## save the plot in the visuals folder\n",
    "        # plt.savefig(os.path.join(visuals_folder, 'feature_importance_plot.png'))\n",
    "        # logger.info('feature imp figure saving is successful')\n",
    "\n",
    "        ## further Splitting the data.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, shuffle = True) \n",
    "        logger.info('final splitting the data is successful')\n",
    "        # logger.info(y_train.shape)\n",
    "        # logger.info(y_test.shape)        \n",
    "        \n",
    "\n",
    "        ## returning splitted data and data_path.\n",
    "        return (\n",
    "            X_train, \n",
    "            X_test, \n",
    "            y_train, \n",
    "            y_test,\n",
    "            self.config.data_path\n",
    "            # self.data_transformation_config.transformed_data_file_path\n",
    "        )    \n",
    "\n",
    "           \n",
    "\n",
    "\n",
    "# class DataTransformation:\n",
    "#     def __init__(self, config: DataTransformationConfig):\n",
    "#         self.config = config\n",
    "\n",
    "    \n",
    "#     ## Note: You can add different data transformation techniques such as Scaler, PCA and all\n",
    "#     #You can perform all kinds of EDA in ML cycle here before passing this data to the model\n",
    "\n",
    "#     # I am only adding train_test_spliting cz this data is already cleaned up\n",
    "\n",
    "\n",
    "#     def train_test_spliting(self):\n",
    "#         data = pd.read_excel(self.config.data_path)\n",
    "\n",
    "#         # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "#         train, test = train_test_split(data)\n",
    "\n",
    "#         train.to_csv(os.path.join(self.config.root_dir, \"train.csv\"),index = False)\n",
    "#         test.to_csv(os.path.join(self.config.root_dir, \"test.csv\"),index = False)\n",
    "\n",
    "#         logger.info(\"Splited data into training and test sets\")\n",
    "#         logger.info(train.shape)\n",
    "#         logger.info(test.shape)\n",
    "\n",
    "#         print(train.shape)\n",
    "#         print(test.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-30 08:55:20,301: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-11-30 08:55:20,304: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-11-30 08:55:20,306: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2023-11-30 08:55:20,314: INFO: common: created directory at: artifacts]\n",
      "[2023-11-30 08:55:20,314: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2023-11-30 08:55:23,037: INFO: 371139583:  data path: \n",
      "artifacts/data_ingestion/flightfare-data.xlsx]\n",
      "[2023-11-30 08:55:23,037: INFO: 371139583: Read data completed]\n",
      "[2023-11-30 08:55:23,037: INFO: 371139583: df dataframe head: \n",
      "       Airline Date_of_Journey    Source Destination                  Route Dep_Time  Arrival_Time Duration Total_Stops Additional_Info  Price\n",
      "0       IndiGo      24/03/2019  Banglore   New Delhi              BLR → DEL    22:20  01:10 22 Mar   2h 50m    non-stop         No info   3897\n",
      "1    Air India       1/05/2019   Kolkata    Banglore  CCU → IXR → BBI → BLR    05:50         13:15   7h 25m     2 stops         No info   7662\n",
      "2  Jet Airways       9/06/2019     Delhi      Cochin  DEL → LKO → BOM → COK    09:25  04:25 10 Jun      19h     2 stops         No info  13882\n",
      "3       IndiGo      12/05/2019   Kolkata    Banglore        CCU → NAG → BLR    18:05         23:30   5h 25m      1 stop         No info   6218\n",
      "4       IndiGo      01/03/2019  Banglore   New Delhi        BLR → NAG → DEL    16:50         21:35   4h 45m      1 stop         No info  13302]\n",
      "[2023-11-30 08:55:23,130: INFO: 371139583: df data transformation completed]\n",
      "[2023-11-30 08:55:23,147: INFO: 371139583:  transformed df data head: \n",
      "   Total_Stops  Price  journey_date  journey_month  Air Asia  Air India  GoAir  IndiGo  Jet Airways  Jet Airways Business  Multiple carriers  Multiple carriers Premium economy  SpiceJet  Vistara  Vistara Premium economy  Chennai  Mumbai  Cochin  Hyderabad  New Delhi duration\n",
      "0            0   3897            24              3         0          0      0       1            0                     0                  0                                  0         0        0                        0        0       0       0          0          1        2\n",
      "1            2   7662             1              5         0          1      0       0            0                     0                  0                                  0         0        0                        0        0       0       0          0          0        3\n",
      "2            2  13882             9              6         0          0      0       0            1                     0                  0                                  0         0        0                        0        0       0       1          0          0        3\n",
      "3            1   6218            12              5         0          0      0       1            0                     0                  0                                  0         0        0                        0        0       0       0          0          0        2\n",
      "4            1  13302             1              3         0          0      0       1            0                     0                  0                                  0         0        0                        0        0       0       0          0          1        2]\n",
      "[2023-11-30 08:55:32,085: INFO: 371139583: transformed data is stored]\n",
      "[2023-11-30 08:55:33,984: INFO: 371139583: final splitting the data is successful]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    # data_transformation.train_test_spliting()\n",
    "    # New Line\n",
    "    data_transformation.initiate_data_transformation()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
